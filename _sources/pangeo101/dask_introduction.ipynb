{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {},
   "source": [
    "# Parallel computing with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bb79b-1b06-42c9-98d5-6185252c86e5",
   "metadata": {},
   "source": [
    "## Authors & Contributors\n",
    "### Authors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "- Anne Fouilloux, Simula Research Laboratory (Norway), [@annefou](https://github.com/annefou)\n",
    "- Pier Lorenzo Marasco, Ispra (Italy), [@pl-marasco](https://github.com/pl-marasco)\n",
    "\n",
    "### Contributors\n",
    "- Guillaume Eynard-Bontemps, CNES (France), [@guillaumeeb](https://github.com/guillaumeeb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is Dask?</li>\n",
    "        <li>How can I parallelize my data analysis with Dask?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about Dask</li>\n",
    "        <li>Learn about Dask Gateway, Dask Client, Scheduler, Workers</li>\n",
    "        <li>Understand out-of-core and speed-up limitations</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "\n",
    "We will be using [Dask](https://docs.dask.org/) with [Xarray](https://docs.xarray.dev/en/stable/) to parallelize our data analysis. The analysis is very similar to what we have done in previous episodes but this time we will use data on a global coverage that we read from a shared catalog (stored online in the Pangeo EOSC Openstack Object Storage).\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using CMIP6 data from intake-esm catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c600794-dd9e-400b-bd09-dbb6e7039dad",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following Python packages:\n",
    "\n",
    "- pooch {cite:ps}`e-pooch-Uieda2020`\n",
    "- s3fs {cite:ps}`e-s3fs-2016`\n",
    "- xarray {cite:ps}`e-xarray-hoyer2017` with [`netCDF4`](https://pypi.org/project/h5netcdf/) and [`h5netcdf`](https://pypi.org/project/h5netcdf/) engines\n",
    "- hvplot {cite:ps}`e-holoviews-rudiger2020`\n",
    "- dask {cite:ps}`e-dask-2016`\n",
    "- graphviz {cite:ps}`e-graphviz-Ellson2003`\n",
    "- numpy {cite:ps}`e-numpy-harris2020`\n",
    "- pandas {cite:ps}`e-pandas-reback2020`\n",
    "- geopandas {cite:ps}`e-geopandas-jordahl2020`\n",
    "\n",
    "Please install these packages if not already available in your Python environment (you might want to take a look at [the Setup page of the tutorial](https://pangeo-data.github.io/clivar-2022/before/setup.html)).\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd73e3-0120-4457-b84b-ccd9325a72a6",
   "metadata": {},
   "source": [
    "## Parallelize with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35db696-501a-4110-a7fa-fafe92dfc0a2",
   "metadata": {},
   "source": [
    "We know from previous chapter [chunking_introduction](./chunking_introduction.ipynb) that chunking is key for analyzing large datasets. In this episode, we will learn to parallelize our data analysis using [Dask](https://docs.dask.org/) on our chunked dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f3544-8b08-462d-b8a4-c6c0c2f68a26",
   "metadata": {},
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** scales the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask or process bigger than memory datasets.\n",
    "\n",
    "- Dask is a flexible library for parallel computing in Python.\n",
    "- It is widely used for handling large and complex Earth Science datasets and speed up science.\n",
    "- Dask is powerful, scalable and flexible. It is the leading platform today for data analytics at scale.\n",
    "- It scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n",
    "- The strength of Dask is that is scales and accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n",
    "\n",
    "It is interesting to note that at first, [Dask has been created to handle data that is larger than memory, on a single computer](https://coiled.io/blog/history-dask/). It then was extended with Distributed to compute data in parallel over clusters of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44c561-da09-4051-b562-bca3c276659e",
   "metadata": {},
   "source": [
    "#### How does Dask scale and accelerate your data analysis?\n",
    "\n",
    "[Dask proposes different abstractions to distribute your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html). In this _Dask Introduction_ section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in pangeo ecosystem as a back end of Xarray.\n",
    "\n",
    "As shown in the [previous section](./chunking_introduction.ipynb) Dask Array is based on chunks.\n",
    "Chunks of a Dask Array are well-known Numpy arrays. By transforming our big datasets to Dask Array, making use of chunk, a large array is handled as many smaller Numpy ones and we can compute each of these chunks independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595d939-199f-4cbd-8107-2b4480a993ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Warning</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>`Xarray` uses Dask Arrays instead of Numpy when chunking is enabled, and thus all Xarray operations are performed through Dask, which enables distributed processing. </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9fd40-08f4-43e2-897f-b1e745fd215a",
   "metadata": {},
   "source": [
    "#### How does Xarray with Dask distribute data analysis?\n",
    "\n",
    "When we use chunks with `Xarray`, the real computation is only done when needed or asked for, usually when invoking `compute()` function. Dask generates a **task graph** describing the computations to be done. When using [Dask Distributed](https://distributed.dask.org/en/stable/) a **Scheduler** distributes these tasks across several **Workers**.\n",
    "\n",
    "![Xarray with dask](../figures/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11377e82-113a-4260-8bc6-cc6973a6bab0",
   "metadata": {},
   "source": [
    "####Â What is a Dask Distributed ?\n",
    "\n",
    "A Dask Distributed is made of two main components:\n",
    "\n",
    "- a Scheduler, responsible for dishandling computations graph and distributing tasks to Workers.\n",
    "- One or several (up to 1000s) Workers, computing individual tasks and storing results and data into distributed memory (RAM and/or worker's local disk).\n",
    "\n",
    "A user usually needs distributed __Client__ and __Cluster__ object as shown below to use Dask Distribute.    \n",
    "\n",
    "![Dask Distributed Cluster](https://user-images.githubusercontent.com/306380/66413985-27111600-e9be-11e9-9995-8f418ff48f8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f6d9c-5927-4d53-9125-432cfe42fef9",
   "metadata": {},
   "source": [
    "#### Where can we deploy Dask distributed cluster?  \n",
    "[Dask distributed clusters can be deployed on your laptop or on distributed infrastructures ( Cloud, HPC centers or ..).](https://docs.dask.org/en/stable/deploying.html)  Dask distributed `Cluster` object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n",
    "\n",
    "![Dask Cluster deployment](https://docs.dask.org/en/stable/_images/dask-cluster-manager.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4ff9d-b390-49f9-a0e6-622170d955af",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "A Dask `Cluster` can also be created on a single machine (for instance your laptop) e.g. there is no need to have dedicated computational resources. However, speedup will only be limited to your single machine resources if you do not have dedicated computational resources!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52be3e-3924-4d0b-9a46-abd9009e2a3b",
   "metadata": {},
   "source": [
    "### Dask distributed Client\n",
    " \n",
    "The Dask distributed `Client` is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a `Client` object. Once a `Client` has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n",
    "\n",
    "No matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregates it back in its underlying Python process.\n",
    "\n",
    "Using `Client()` function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the 'local' machine. Here, during the workshop, we are running this notebook in Pangeo EOSC cloud deployment, so the 'local' machine is the jupyterlab you are using at the Cloud, and the number of cores is the number of cores on the cloud computing resources you've been given (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879754c-8ede-43ed-86ef-05cd9c288e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # create a local dask cluster on the local machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eb7fa-76b6-4a22-a139-3eb53989645d",
   "metadata": {},
   "source": [
    "Inspecting the `Cluster Info` section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736568ed-7e5b-4184-bf7a-26e1d2426f93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> You can also create a local cluster with the `LocalCluster` constructor and use `n_workers` \n",
    "        and `threads_per_worker` to manually specify the number of processes and threads you want to use. \n",
    "        For instance, we could use `n_workers=2` and `threads_per_worker=2`.  </li>\n",
    "        <li> This is sometimes preferable (in terms of performance), or when you run this tutorial on your PC, \n",
    "        you can avoid dask to use all your resources you have on your PC!  </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d1b16-99b6-4b21-939d-b4a3bf264cbf",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "\n",
    "Dask comes with a really handy interface: the Dask Dashboard.\n",
    "We will learn here, how to use that through [dask jupyterlab extension](https://github.com/dask/dask-labextension).  \n",
    "\n",
    "To use Dask Dashbord through jupyterlab extension at Pangeo EOSC FOSS4G infrastructure,\n",
    "you will just need too look at the html link you have for your jupyterlab, and Dask dashboard port number, as highlighted in the figure below.\n",
    "\n",
    "\n",
    "<img src=\"../figures/dashboardlink.png\" width=\"50%\">\n",
    "<img src=\"../figures/dasklab.png\" width=\"30%\">\n",
    "\n",
    "Then click the orange icon indicated in the above figure, and type 'your' dashboard link (normally, you just need to replace 'todaka' to 'your username').  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can click several buttons indicated with blue arrows in above figures, then drag and drop to place them as your convenience.  \n",
    "\n",
    "\n",
    "<img src=\"../figures/exampledasklab.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "It's really helpfull to understand your computation and how it is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171f9b8-eb49-43ee-9d3f-5b8d74434e1f",
   "metadata": {},
   "source": [
    "## Dask Distributed computations on our dataset\n",
    "\n",
    "Lets open dataset from catalogue we made before, select a single location over time and visualize the task graph generated by Dask, and observe the Dask Dashboard.\n",
    "### Read from online cmip6 ARCO dataset\n",
    "\n",
    " We will access analysis-ready, cloud optimized (ARCO) introduced in [previous data_discovery](./data_discovery.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d552c-d986-465c-ab72-578432d579c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "xr.set_options(display_style=\"html\")\n",
    "import intake\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import hvplot.xarray\n",
    "\n",
    "%matplotlib inline\n",
    "cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(cat_url)\n",
    "cat = col.search(\n",
    "    experiment_id=[\"historical\"]\n",
    "    # , table_id=['day']\n",
    "    # source_id=['NorESM2-LM']\n",
    "    ,\n",
    "    institution_id=[\"NIMS-KMA\"]\n",
    "    # , table_id=['Amon']\n",
    "    ,\n",
    "    table_id=[\"3hr\"],\n",
    "    variable_id=[\"tas\"],\n",
    "    member_id=[\"r1i1p1f1\"],\n",
    "    grid_label=[\"gr\"],\n",
    ")\n",
    "display(cat.df)\n",
    "dset_dict = cat.to_dataset_dict(zarr_kwargs={\"use_cftime\": True})\n",
    "dset = dset_dict[list(dset_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b9e2f-ceef-41a8-a3b3-76f4848b3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4d9cb-0078-474b-90d6-8a85dab0083b",
   "metadata": {},
   "source": [
    "## Compute Weigted arctic average\n",
    "Let's try to also take only the data above 60$^\\circ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a032ef-4526-46f2-8e7f-2af5259006bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dset.where(dset[\"lat\"] > 60.0, drop=True)  #\n",
    "# this line limiting time is only for building jupyter book\n",
    "dset = dset.sel(time=slice(\"2000-01-01\", \"2010-01-01\"))\n",
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e050c-6182-406b-aa40-9c17ee77f834",
   "metadata": {},
   "source": [
    "By inspecting the variable 'tas' on the representation above, you'll see that data array represent __about 8.2 GiB of data__, so more thant what is available on this notebook server, i.e. even on the local Dask Cluster we created above. But thanks to chunking, we can still analyze it!\n",
    "\n",
    "Lets plot the first timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7190e-5596-413c-a066-cfe214e933f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "projection = ccrs.Mercator(central_longitude=-10)\n",
    "\n",
    "f, ax = plt.subplots(subplot_kw=dict(projection=projection))\n",
    "\n",
    "dset[\"tas\"].isel(time=0).plot(\n",
    "    transform=ccrs.PlateCarree(), cbar_kwargs=dict(shrink=0.7), cmap=\"coolwarm\"\n",
    ")\n",
    "ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb068274-4250-48e8-b84b-e0c0251612ee",
   "metadata": {},
   "source": [
    "## Compute weighted mean\n",
    "\n",
    "1. Creating weights: for a rectangular grid the cosine of the latitude is proportional to the grid cell area.\n",
    "2. Compute weighted mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0aa14-0edb-46e1-8ddc-b04b603b46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeWeightedMean(ds):\n",
    "    # Compute weights based on the xarray you pass\n",
    "    weights = np.cos(np.deg2rad(ds.lat))\n",
    "    weights.name = \"weights\"\n",
    "    # Compute weighted mean\n",
    "    air_weighted = ds.weighted(weights)\n",
    "    weighted_mean = air_weighted.mean((\"lon\", \"lat\"))\n",
    "    return weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755f17d-4c89-419b-9cdf-4e9158c18c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean = computeWeightedMean(dset)\n",
    "weighted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b90d-22d6-404f-8c3d-3c557410d601",
   "metadata": {},
   "source": [
    "Did you notice something on the Dask Dashboard when running the two previous cells?\n",
    "\n",
    "We didn't 'compute' anything. We just build a Dask task graph with it's size indicated as count above, but did not ask Dask to return a result.\n",
    "\n",
    "\n",
    "You can try to plot the dask graph before computation and understand what dask workers will do to compute the value we asked for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45148205-16b9-42a4-aaba-938c5cea593c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weighted_mean.tas.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698f61f-32bf-4ccc-bfcf-2f58de316431",
   "metadata": {},
   "source": [
    "### Lets compute using dask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7d0dc-45e1-49f8-8d8a-ac7f212d00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weighted_mean = weighted_mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6539a-0433-481b-a080-6e9eb7b80c1d",
   "metadata": {},
   "source": [
    "Calling compute on our Xarray object triggered the execution on Dask Cluster side. \n",
    "\n",
    "You should be able to see how Dask is working on Dask Dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06b6f5-93f5-416d-bc04-34d781b27b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean.tas.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4e8f1-fc6b-4c6f-b7bb-b7a61a3d944d",
   "metadata": {},
   "source": [
    "### Close client to terminate local dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8d36a-a4cd-434c-a1bf-cc1850d2a15a",
   "metadata": {},
   "source": [
    "The `Client` and associated `LocalCluster` object will be automatically closed when your Python session ends. When using Jupyter notebooks, we recommend to close it explicitely whenever you are done with your local Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11f2cf-7418-43de-9b45-847556474b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e06ec-f40a-4ed4-b0b0-61db5232e115",
   "metadata": {},
   "source": [
    "## Scaling your Computation using Dask Gateway.\n",
    "\n",
    "For this workshop, according to the Pangeo EOSC deployment, you will learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing to run our data analysis in parallel e.g. distribute tasks across several workers.\n",
    "\n",
    "Lets set up your Dask Gateway.  \n",
    "As Dask Gateway is configured by default on this ifnrastructure, you just need to execute the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab00293-fc51-409b-8bb6-10a6e04c7249",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7962b-dd1f-4b18-8970-2147cc30eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "##A line of trick to clean your dask cluster before you start your computation\n",
    "from dask.distributed import Client\n",
    "\n",
    "clusters = gateway.list_clusters()\n",
    "print(clusters)\n",
    "for cluster in clusters:\n",
    "    cluster = gateway.connect(cluster.name)\n",
    "    print(cluster)\n",
    "    client = Client(cluster)\n",
    "    client.close()\n",
    "    cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980455f-e639-47e9-8d53-5f47b00370d2",
   "metadata": {},
   "source": [
    "### Create a new Dask cluster with the Dask Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79047f2-044a-41c3-b8f0-4acbd121fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please don't execute this cell, it is needed for building the Jupyter Book\n",
    "cluster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624b389-1add-4476-8aa2-58087c409571",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(worker_memory=2, worker_cores=1)\n",
    "\n",
    "cluster.scale(8)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78693fbb-c012-47da-8a7b-4f2c7256f9e1",
   "metadata": {},
   "source": [
    "Lets setup the Dask Dashboard with your new cluster.  \n",
    "This time, copy and past the link above indicated in Dashboard to the dasklab extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3742e-2d4b-48a3-a2da-ee67d019dc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get a client from the Dask Gateway Cluster\n",
    "\n",
    "As stated above, creating a Dask `Client` is mandatory in order to perform following Daks computations on your Dask Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f05e7-0827-4f8a-a6d3-a8c8abfbf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "if cluster:\n",
    "    client = Client(cluster)  # create a dask Gateway cluster\n",
    "else:\n",
    "    client = Client()  # create a local dask cluster on the machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4818f-2b6b-44c1-af77-97daf8a1c2a1",
   "metadata": {},
   "source": [
    "## Repeat above computation with more dask workers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d6498-4dd9-4998-886d-a2ddab5864fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89018c-ade6-4e73-a02d-c98347089a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weighted_mean = computeWeightedMean(dset)\n",
    "weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6524f7-1c03-4745-85df-b90280890cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weighted_mean = weighted_mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95657473-11df-495e-81f5-4e8f6c0d18dc",
   "metadata": {},
   "source": [
    "## Comparison with unweighted mean\n",
    "- We select a time range\n",
    "- Note how the weighted mean temperature is higher than the unweighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944905bc-bbad-4140-bf63-4ceba95af2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean[\"tas\"].sel(time=slice(\"2000-01-01\", \"2010-01-01\")).plot(label=\"weighted\")\n",
    "dset[\"tas\"].sel(time=slice(\"2000-01-01\", \"2010-01-01\")).mean((\"lon\", \"lat\")).plot(\n",
    "    label=\"unweighted\"\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56545676-100a-4acd-82e0-70f661f8a956",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Exercise</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> It was faster.  Why? </li>          \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483e8cc-62c0-4396-8454-0e30833b4499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96e1914e-c9fa-456c-9612-f2ef41613074",
   "metadata": {},
   "source": [
    "## hvplot and dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bd274-1f41-42cc-a2fc-ed0b8fd1d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.isel(member_id=0).hvplot.quadmesh(  # time=0,\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    rasterize=True,\n",
    "    geo=True,\n",
    "    global_extent=False,\n",
    "    projection=ccrs.Orthographic(30, 90),\n",
    "    project=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    coastline=\"50m\",\n",
    "    frame_width=400,\n",
    "    title=dset.attrs[\"references\"]\n",
    "    # , title=\"Near-surface Temperature over Norway (CMIP6 CESM2)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05dbb2-42b5-4973-be58-3b5d7c5e5b9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go Further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Compare the data size and 'used' data size for each worker in dask dashboard  </li>   \n",
    "        <li>Lets try to zoom.  What happend with your plot? How was the dask dashboard reacted with zooming?   </li>   \n",
    "        <li>What is rastersize=True ? (Hint: https://hvplot.holoviz.org/user_guide/Customization.html#datashading-options)   </li>  \n",
    "        <li>What is 'cluster.scale(4)' ? </li>\n",
    "        <li>instead of .compute, try using .persist, how's the dashboard?\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67cb31-b8f2-4d91-a37f-95d2a4ca328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499dae2-f69e-4764-b1fd-fb3aef29e29d",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a4ec6-bbf8-45c5-92d3-4c2ff2b0d119",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go Further </b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Dask will try to hold data on the memory, then try to spill that to hard disk of worker.  If you would like to avoid that dask worker use your local disk (It may slow down your computation depending on what kind of hardware you have), you can use following command after importing dask.distributed. </li>          \n",
    "        dask.config.set({\"distributed.worker.memory.spill\": 0})\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56304d1c",
   "metadata": {},
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"dask\" and topic % \"package\"\n",
    ":keyprefix: e-\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
