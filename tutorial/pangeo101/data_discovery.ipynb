{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data access and discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Authors & Contributors\n",
    "\n",
    "### Authors\n",
    "\n",
    "- Pier Lorenzo Marasco, Ispra (Italy), [@pl-marasco](https://github.com/pl-marasco)\n",
    "- Alejandro Coca-Castro, The Alan Turing Institute (United Kingdom), [@acocac](https://github.com/acocac)\n",
    "- Anne Fouilloux, Simula Research Laboratory (Norway), [@annefou](https://github.com/annefou)\n",
    "\n",
    "### Contributors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Overview</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>How to access online (remote) datasets?</li>\n",
    "        <li>How to prepare and discover online geoscience datasets?</li>\n",
    "        <li>What is Analysis Ready, Cloud Optimized data (ARCO)?</li>\n",
    "        <li>What is pangeo-forge?</li>\n",
    "        <li>What is stac?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn to access datasets from online object storage</li>\n",
    "        <li>Learn about preparing and discovery online datasets</li>\n",
    "        <li>Learn about Analysis Cloud Optimized (ARCO) data</li>\n",
    "        <li>Learn about the Pangeo Forge initiative</li>\n",
    "        <li>Learn about stac</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "We will be using CMIP6 data and access them through [S3-compatible storage](https://en.wikipedia.org/wiki/Amazon_S3) using [Pangeo Catalog](https://pangeo-data.github.io/pangeo-cmip6-cloud/pangeo_catalog.html). \n",
    "\n",
    "We will also discuss the generation of Cloud-Optimised Datasets by introducing the [pangeo-forge](https://pangeo-forge.org) initiative. Finally, we will explore [Sentinel-2 Cloud-Optimised Dataset](https://registry.opendata.aws/sentinel-2-l2a-cogs/) online through SpatioTemporal Asset Catalogs ([STAC](https://stacspec.org/en))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following main Python packages:\n",
    "\n",
    "- xarray {cite:ps}`c-xarray-hoyer2017` with [`netCDF4`](https://pypi.org/project/h5netcdf/) and [`h5netcdf`](https://pypi.org/project/h5netcdf/) engines\n",
    "- intake-esm {cite:ps}`c-intake-esm`\n",
    "- s3fs {cite:ps}`c-s3fs-2016`\n",
    "\n",
    "Please install these packages if not already available in your Python environment.\n",
    "\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Introduction to CMIP6 data\n",
    "\n",
    "Coupled Model Intercomparison project Phase 6\n",
    "- Project under World Climate Research Programme (WCRP) \n",
    "- Since 1995 CMIP has coordinated climate model experiments \n",
    "- Defines common experiment protocols, forcings and output. \n",
    "- 33 model groups participate\n",
    "\n",
    "\n",
    "#### Very useful links:\n",
    "- Database for data request: http://clipc-services.ceda.ac.uk/dreq/index.html\n",
    "    - **-->SEARCH FOR VARIABLES**: http://clipc-services.ceda.ac.uk/dreq/mipVars.html\n",
    "    - Search for experiments: http://clipc-services.ceda.ac.uk/dreq/experiments.html\n",
    "- Overview: https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6\n",
    "- ES-DOCs: https://search.es-doc.org/\n",
    "\n",
    "\n",
    "#### Other links:\n",
    "GMD special issue with articles explaining all MIPs in CMIP6 :\n",
    "https://www.geosci-model-dev.net/special_issue590.html\n",
    "\n",
    "- General CMIP6 website https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6\n",
    "- Guidance documents: (https://pcmdi.llnl.gov/CMIP6/)\n",
    "- Emissions/Forcing datasets (https://esgf-node.llnl.gov/projects/input4mips/)\n",
    "- Participating Modelling Groups (https://rawgit.com/WCRP-CMIP/CMIP6_CVs/master/src/CMIP6_institution_id.html)\n",
    "- Model and experiment documentation (https://search.es-doc.org/)\n",
    "- CMIP6 ESMValTool evaluation and analysis results (https://cmip-esmvaltool.dkrz.de/)\n",
    "\n",
    "- Emission visualising: https://eccad.aeris-data.fr\n",
    "\n",
    "\n",
    "#### Advantages:\n",
    "- Homogenized and standardized outputs\n",
    "- Same variable name\n",
    "- Same experiments\n",
    "\n",
    "#### Experiments (DECK)\n",
    "![image](https://user-images.githubusercontent.com/17406708/139691209-ec237004-637b-4947-bb12-104d78a2fe44.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### S3-compatible Object Storage to access online data\n",
    "\n",
    "Up to now we have downloaded data locally and then opened with Xarray `open_dataset`. When willing to manipulate large amount of data, this approach is not optimal (since it requires a lot of unnecessary local downloads). Sharing data online as Object Storage allows for data sharing and access to much larger amounts of data.\n",
    "\n",
    "One of the most popular methods to access online remote data is through Amazon Simple Storage Service (S3) and you don't necessarily need to use Amazon services to benefit from S3 object storage. Many other providers offer S3-compatible object storage that can be accessed in a very similar way.\n",
    "\n",
    "When using S3-compatible object storage, you still need to list all the files you would like to access. With such amount of data, it would be very cumbersome. This is why a catalog is created: this catalog is a text file (`json` format) which describes where and which data to get. It adds additional metadata too. CMIP6 Pangeo online catalog can be loaded using `intake-esm` Python package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Introduction to the Pangeo CMIP6 online catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "xr.set_options(display_style=\"html\")\n",
    "import intake\n",
    "import cftime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Open CMIP6 online catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(cat_url)\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Search corresponding data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(\n",
    "    source_id=[\"CESM2\"],\n",
    "    experiment_id=[\"historical\"],\n",
    "    table_id=[\"Amon\"],\n",
    "    variable_id=[\"tas\"],\n",
    "    member_id=[\"r1i1p1f1\"],\n",
    ")\n",
    "cat.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Create dictionary from the list of datasets we found\n",
    "- This step may take several minutes so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = cat.to_dataset_dict(zarr_kwargs={\"use_cftime\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = list(dset_dict.keys())\n",
    "dataset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Open dataset\n",
    "\n",
    "- Use `xarray` python package to analyze netCDF dataset\n",
    "- `open_dataset` allows to get all the metadata without loading data into memory. \n",
    "- with `xarray`, we only load into memory what is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dset_dict[dataset_list[0]]\n",
    "dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Get metadata corresponding to near-surface air temperature (tas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset[\"tas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.sel(lat=60, lon=10.75, method=\"nearest\").sel(time=\"1850-12-15\")[\"tas\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "The same dataset can be available from different locations e.g. ESGF, Zenodo, S3-compatible Object storage, etc.\n",
    "How do you know if it corresponds to the very same dataset? You cannot know except if the datasets have a persistent identifier such as a Digital Object Identifier.\n",
    "It is therefore recommended *1)* to be extra careful about where you get your datasets, and *2)* to double check that the content is exactly what you expect (for instance, you can perform basic quality checks).                                                            \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Access remote files from S3-compatible Object Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The Pangeo CMIP6 online catalog is very user-friendly. However, the complete ESGF CMIP6 catalog is close to 20 PB and so far only a very tiny amount of cloud-optimised CMIP6 data has been generated (approximately 1PB).\n",
    "Therefore, you may have to download CMIP6 data or other kind of data for instance when comparing CMIP6 model outputs with observations.\n",
    "\n",
    "Rather than downloading datasets locally and individually, it is recommended to share what you download to everyone. Additional datasets are being stored and made \n",
    "publicly available in OpenStack Object storage (Swift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    anon=True, client_kwargs={\"endpoint_url\": \"https://object-store.cloud.muni.cz\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "The parameter `anon` is for `anonymous` and is set to `True` because the data we have stored at `https://object-store.cloud.muni.cz` is public\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### List files and folders in existing buckets\n",
    "\n",
    "Instead of organizing files in various folders, object storage systems store files in a flat organization of containers (called \"buckets\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"MODIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls(\"MODIS/MOD08_M3.A2000183.061.2017276075622.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Access remote files from S3-compatible Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = \"s3://MODIS/MOD08_M3.A2000183.061.2017276075622.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis = xr.open_dataset(fs.open(s3path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Rename dimensions and variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis = modis.rename_dims(\n",
    "    {\n",
    "        \"YDim:mod08\": \"lat\",\n",
    "        \"XDim:mod08\": \"lon\",\n",
    "        \"Effective_Optical_Depth_Average_Ocean_Micron_Levels:mod08\": \"levels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis = modis.rename_vars(\n",
    "    {\n",
    "        \"YDim\": \"lat\",\n",
    "        \"XDim\": \"lon\",\n",
    "        \"Effective_Optical_Depth_Average_Ocean_Micron_Levels\": \"levels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = modis.lon.squeeze().reset_coords(drop=True)\n",
    "y = modis.lat.squeeze().reset_coords(drop=True)\n",
    "z = modis.levels.squeeze().reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis = modis.assign_coords({\"lon\": x, \"lat\": y, \"levels\": z})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis.sel(lat=60, lon=10.75, method=\"nearest\")[\n",
    "    \"Aerosol_Optical_Depth_Land_Ocean_Mean_Mean\"\n",
    "].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#### Access multiple remote files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = \"s3://MODIS/*.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_files = fs.glob(s3path)\n",
    "remote_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We need to add a time dimension to concatenate data. For this, we define a function that will be called for each remote file (via the `preprocess` parameter of Xarray `open_mfdataset`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_datetimeindex(paths):\n",
    "    return [\n",
    "        datetime.strptime(date.split(\".A\")[-1].split(\".\")[0], \"%Y%j\") for date in paths\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Xarray `open_mfdataset` allows opening multiple files at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through remote_files to create a fileset\n",
    "fileset = [fs.open(file) for file in remote_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "When opening remote files, you can also select the variables you wish to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create variable used for time axis\n",
    "time_var = xr.Variable(\"time\", paths_to_datetimeindex(remote_files))\n",
    "time_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in and concatenate all individual GeoTIFFs\n",
    "modis = xr.concat(\n",
    "    [\n",
    "        xr.open_mfdataset(\n",
    "            [i],\n",
    "        )\n",
    "        for i in fileset\n",
    "    ],\n",
    "    dim=time_var,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Apply the same pre-processing as earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modis = modis.rename_dims(\n",
    "    {\n",
    "        \"YDim:mod08\": \"lat\",\n",
    "        \"XDim:mod08\": \"lon\",\n",
    "        \"Effective_Optical_Depth_Average_Ocean_Micron_Levels:mod08\": \"levels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis = modis.rename_vars(\n",
    "    {\n",
    "        \"YDim\": \"lat\",\n",
    "        \"XDim\": \"lon\",\n",
    "        \"Effective_Optical_Depth_Average_Ocean_Micron_Levels\": \"levels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis = modis.assign_coords({\"longitude\": x, \"latitude\": y, \"levels\": z})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "If you use one of xarray’s open methods such as xarray.open_dataset to load netCDF files with the default engine, it is recommended to use decode_coords=”all”. This will load the grid mapping variable into coordinates for compatibility with rioxarray. See [rioxarray documentation](https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html#xarray).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Preparing and discover online datasets\n",
    "\n",
    "With the plethora of cloud storage, there are many available online datasets. To ease the preparation and discovery of such datasets, we describe emerging community-driven initiatives promoting standards suited to both geospatial and geoscience communities. Most of the material below is adapted from a previous Pangeo 101 training {cite:ps}`galaxy2022-pangeo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    ":::{tip}\n",
    "While we provide a general intro to some initiatives, we suggest below a list of FOSS4G 2022 talks with very interesting developments to prepare and discover spatio-temporal datasets in the cloud. Enjoy!\n",
    "\n",
    "- [STAC Best Practices and Tools](https://talks.osgeo.org/foss4g-2022/talk/9RRYZM/), 2022-08-24, 11:00–11:30\n",
    "- [Early use of FOSS4G in a space start up](https://talks.osgeo.org/foss4g-2022/talk/HG7RLR/), 2022-08-24, 11:30–12:00\n",
    "- [Exploring Data Interoperability with STAC and the Microsoft Planetary Computer](https://talks.osgeo.org/foss4g-2022/talk/L3KNY8/), 2022-08-24, 12:10–12:15\n",
    "- [Serving oblique aerial imagery using STAC and Cloud Optimized Geotiffs](https://talks.osgeo.org/foss4g-2022/talk/SQYE9A/), 2022-08-24, 14:45–15:15\n",
    "- [Pangeo Forge: Crowdsourcing Open Data in the Cloud](https://talks.osgeo.org/foss4g-2022/talk/DABTGG/). 2022-08-26, 10:00-10:30.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Analysis Ready, cloud optimized data (ARCO)\n",
    "When analyzing data at scale, the data format used is key. For years, the main data format was netCDF e.g. Network Common Data Form but with the use of cloud computing and interest in Open Science, different formats are often more suitable.\n",
    "\n",
    "Formats for analyzing data from the cloud are refered to as \"Analysis Ready, Cloud Optimized\" data formats or in short ARCO. Find further info about ARCO datasets in {cite:ps}`Abernathey2022-arco`.\n",
    "\n",
    "What is \"Analysis Ready\"?\n",
    "* Think in terms of \"Datasets\" not \"data files\"\n",
    "* No need for tedious homogenizing / cleaning setup guides\n",
    "* Curated and cataloged\n",
    "\n",
    "What is \"Cloud Optimized\"?\n",
    "* Compatible with object storage e.g. access via HTTP\n",
    "* Supports lazy access and intelligent subsetting\n",
    "* Integrates with high-level analysis libraries and distributed frameworks\n",
    "\n",
    "Instead of having a big dataset, ARCO datasets are chunked appropriately for analysis and have rich metadata (See Figure 1).\n",
    "\n",
    "<img src=\"https://github.com/galaxyproject/training-material/blob/696dfecd4c88e59b487a7a3557cfedca6ec5754b/topics/climate/images/arco_data.png?raw=true\" align=\"Left\" /></a>\n",
    "\n",
    "*Fig 1. Example of an ARCO dataset. Source: {cite:ps}`galaxy2022-pangeo`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### The Pangeo forge initiative\n",
    "\n",
    "[Pangeo Forge](https://pangeo-forge.org/) is an open source platform for data **Extraction**, **Transformation**, and **Loading** (ETL). The goal of *Pangeo Forge* is to make it easy to extract data from traditional repositories and deposit this data in cloud object storage in an analysis-ready, cloud optimized (ARCO) format {cite:ps}`galaxy2022-pangeo`.\n",
    "\n",
    "Pangeo Forge is inspired directly by Conda Forge, a community-led collection of recipes for building conda packages.\n",
    "\n",
    "It is under active development and the Pangeo community hopes it will play a role in democratizing the publication of datasets in ARCO format.\n",
    "\n",
    "#### How does Pangeo Forge work?\n",
    "\n",
    "Pangeo Forge defines the concept of a recipe, which specifies the logic for transforming a specific data archive into an ARCO data store.\n",
    "All contributions to Pangeo Forge must include an executable Python module, named recipe.py or similar, in which the data transformation logic is embedded (Figure 2).\n",
    "The recipe contributor is expected to use one of a predefined set of template algorithms defined by Pangeo Forge.\n",
    "Each of these templated algorithms is designed to transform data of a particular source type into a corresponding ARCO format, and requires only that the contributor populate the template with information unique to their specific data transformation, including the location of the source files and the way in which they should be aligned in the resulting ARCO data store {cite:ps}`Abernathey2022-arco`.\n",
    "\n",
    "The diagram below looks complicated but like for conda forge most of the process is automated.\n",
    "\n",
    "<img src=\"https://www.frontiersin.org/files/Articles/782909/fclim-03-782909-HTML/image_m/fclim-03-782909-g002.jpg\" align=\"Left\" /></a>\n",
    "\n",
    "*Fig 2. A recipe in relation to Pangeo Forge architecture. Source: {cite:ps}`Abernathey2022-arco`.*\n",
    "\n",
    "The next step after preparing the dataset is then to tell the community where and how to access to your transformed dataset.\n",
    "\n",
    "This is done by creating a catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Spatio Temporal Asset Catalogs (STAC)\n",
    "\n",
    "The [STAC](https://stacspec.org/en/) specification is a common language to describe geospatial information, so it can more easily be worked with, indexed, and discovered.\n",
    "\n",
    "#### Why STAC?\n",
    "* Each provider has its own catalog and interface (APIs).\n",
    "* Every time you want to access a new catalog, you need to change your program.\n",
    "* We have lots of data providers and each with a bespoke interface.\n",
    "* It is becoming quickly difficult for programmers who need to design a new data connector each time.\n",
    "\n",
    "#### Features\n",
    "- STAC catalogs are extremely simple.\n",
    "- They are composed of three layers:\n",
    "    - **Catalogs**\n",
    "        - **Collections**\n",
    "            - **Items**\n",
    "- STAC is very popular for Earth Observation satellite imagery.\n",
    "- For instance it can be used to access Sentinel-2 in AWS (see Figure 3).\n",
    "\n",
    "<img src=\"https://github.com/galaxyproject/training-material/blob/696dfecd4c88e59b487a7a3557cfedca6ec5754b/topics/climate/images/sentinel2_AWS.png?raw=true\" align=\"Left\" /></a>\n",
    "\n",
    "*Fig 3. Example of STAC collection of Sentinel-2 images hosted in AWS.\n",
    "Source: {cite:ps}`galaxy2022-pangeo`.*\n",
    "\n",
    "\n",
    "#### STAC and Pangeo Forge\n",
    "- Pangeo-forge supports the creation of analysis-ready cloud optimized (ARCO) data in cloud object storage from \"classical\" data repositories.\n",
    "- STAC is used to create catalogs and goes beyond the Pangeo ecosystem.\n",
    "- Work is ongoing to figure out the best way to expose Pangeo-Forge-generated data assets via STAC catalogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ":::{tip}\n",
    "Pangeo members, Scott Henderson (University of Washington) and Tom Augspurger (Microsoft), provided a great workshop in FOSS4G 2021 covering STAC.\n",
    "\n",
    "Feel free to explore the GitHub repository of the  [here](https://github.com/pangeo-data/foss4g-2021).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Key Points</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Access to remote dataset</li>\n",
    "        <li>ARCO datasets</li>\n",
    "        <li>Pangeo Forge</li>\n",
    "        <li>STAC</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"data-access\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"access\" and topic % \"package\"\n",
    ":keyprefix: c-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
